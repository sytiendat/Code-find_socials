{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "erJOvTDmD-kG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5090f2-5227-4338-821a-b677558b64c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.0-py3-none-any.whl (475 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 trio-0.26.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Collecting g4f\n",
            "  Downloading g4f-0.3.2.2-py3-none-any.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.9/611.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from g4f) (2.31.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from g4f) (3.9.5)\n",
            "Collecting brotli (from g4f)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodome (from g4f)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->g4f) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->g4f) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->g4f) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->g4f) (2024.7.4)\n",
            "Installing collected packages: brotli, pycryptodome, g4f\n",
            "Successfully installed brotli-1.1.0 g4f-0.3.2.2 pycryptodome-3.20.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 44 not upgraded.\n",
            "Need to get 28.5 MB of archives.\n",
            "After this operation, 117 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.63+22.04 [25.9 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 28.5 MB in 2s (17.2 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123586 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.3_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123794 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.63+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.63+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.63+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124024 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  chromium-chromedriver\n",
            "0 upgraded, 1 newly installed, 0 to remove and 44 not upgraded.\n",
            "Need to get 2,308 B of archives.\n",
            "After this operation, 77.8 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Fetched 2,308 B in 0s (13.7 kB/s)\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "(Reading database ... 124045 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n"
          ]
        }
      ],
      "source": [
        "# @title Install the library\n",
        "!pip install selenium\n",
        "!pip install beautifulsoup4\n",
        "!pip install g4f\n",
        "!apt-get install -y chromium-browser\n",
        "!apt install chromium-chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_DqyCGiFZya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7583bbf-fc6e-42e5-b2af-7189f79626b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting curl_cffi\n",
            "  Downloading curl_cffi-0.7.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/6.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/6.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4.9/6.1 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from curl_cffi) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.10/dist-packages (from curl_cffi) (2024.7.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12.0->curl_cffi) (2.22)\n",
            "Installing collected packages: curl_cffi\n",
            "Successfully installed curl_cffi-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install curl_cffi\n",
        "# @title Import library\n",
        "import tkinter as tk\n",
        "import webbrowser  # Thêm thư viện webbrowser\n",
        "from selenium import webdriver\n",
        "import selenium\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from openpyxl import Workbook, load_workbook\n",
        "from bs4 import BeautifulSoup\n",
        "from tkinter import filedialog\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import g4f\n",
        "import random\n",
        "from urllib.parse import urlsplit\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from collections import deque\n",
        "from termcolor import colored\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import spacy\n",
        "# from spacy.lang.en import English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9bCHAA4-qMk"
      },
      "outputs": [],
      "source": [
        "# @title Set up Driver\n",
        "def web_driver():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--verbose\")\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    options.add_argument(\"--window-size=1920, 1200\")\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    return driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMs59j3uVHhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7acc7ce6-5d3e-45f1-fe59-1869c78bae11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Aura', 'GeminiProChat', 'You']\n",
            "['AI365VIP', 'AItianhuSpace', 'Aura', 'Bing', 'BingCreateImages', 'Blackbox', 'Chatgpt4Online', 'Chatgpt4o', 'ChatgptFree', 'DDG', 'DeepInfra', 'DeepInfraImage', 'FlowGpt', 'FreeChatgpt', 'FreeGpt', 'Gemini', 'GeminiPro', 'GeminiProChat', 'GigaChat', 'GptTalkRu', 'Groq', 'HuggingChat', 'HuggingFace', 'Koala', 'Liaobots', 'Local', 'MetaAI', 'MetaAIAccount', 'MyShell', 'Ollama', 'OpenRouter', 'Openai', 'OpenaiAccount', 'OpenaiChat', 'PerplexityAi', 'PerplexityApi', 'PerplexityLabs', 'Pi', 'Pizzagpt', 'Poe', 'Raycast', 'Reka', 'Replicate', 'ReplicateHome', 'TalkAi', 'Theb', 'ThebApi', 'Vercel', 'WhiteRabbitNeo', 'You']\n"
          ]
        }
      ],
      "source": [
        "# @title Providers of g4f\n",
        "g4f.debug.logging = True  # Enable logging\n",
        "g4f.check_version = False  # Disable automatic version checking\n",
        "\n",
        "\n",
        "# Create a list of method references\n",
        "providers = [\n",
        "    g4f.Provider.ChatBase,\n",
        "    g4f.Provider.Bard,\n",
        "    # g4f.Provider.Aichat,\n",
        "    # g4f.Provider.AiChatOnline,\n",
        "    g4f.Provider.Aura,\n",
        "    g4f.Provider.GptForLove,\n",
        "    g4f.Provider.Hashnode,\n",
        "    g4f.Provider.GeminiProChat,\n",
        "    # g4f.Provider.ChatForAi,\n",
        "    # g4f.Provider.ChatgptAi,\n",
        "    g4f.Provider.Gpt6,\n",
        "    g4f.Provider.GptGo,\n",
        "    g4f.Provider.You,\n",
        "    g4f.Provider.Yqcloud,\n",
        "]\n",
        "\n",
        "providers = [provider for provider in providers if provider.working]\n",
        "\n",
        "# Show all providers of g4f, need to see another providers? Let's run and take a look in terminal\n",
        "print([provider.__name__ for provider in providers])\n",
        "print([provider.__name__ for provider in g4f.Provider.__providers__ if provider.working])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an1P1pAlYWH0"
      },
      "outputs": [],
      "source": [
        "# @title Clear special characters\n",
        "def clean_input(input_str):\n",
        "    # Loại bỏ các ký tự đặc biệt, chỉ giữ lại ký tự chữ cái và dấu cách\n",
        "    cleaned_str = re.sub(r\"[^a-zA-Z\\s]\", \"\", input_str)\n",
        "    return cleaned_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlySeeUq6m9w"
      },
      "outputs": [],
      "source": [
        "# @title Upload file\n",
        "def upload_file():\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded [fn])))\n",
        "\n",
        "    file_path = next(iter(uploaded))\n",
        "    print(f\"path: {file_path}\")\n",
        "\n",
        "\n",
        "  return file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcOMthd3kNDL"
      },
      "outputs": [],
      "source": [
        "# @title Find CEO name\n",
        "def find_CEO_name():\n",
        "    ceo_name = \"\"\n",
        "    att = 0\n",
        "    try:\n",
        "        ceo_name_f = driver.find_element(By.CLASS_NAME, \"FLP8od\")\n",
        "        get_CEO_name = ceo_name_f.get_attribute(\"outerHTML\")\n",
        "        soup = BeautifulSoup(get_CEO_name, \"html.parser\")\n",
        "        ceo_name = soup.get_text(strip=True)\n",
        "        print(ceo_name)\n",
        "        att += 1\n",
        "        if len(ceo_name.strip()) > 0:\n",
        "            return ceo_name\n",
        "        elif att == 3:\n",
        "            # ceo_name = f\"Ceo name of {company_name} not found\"\n",
        "            return ceo_name\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error: {str(e)}\"\n",
        "        print(f\"Error: Not found element FLP8od\")\n",
        "        try:\n",
        "            n = []\n",
        "            ceo_name_f = driver.find_element(By.CLASS_NAME, \"hgKElc\")\n",
        "            get_CEO_name = ceo_name_f.get_attribute(\"outerHTML\")\n",
        "            soup = BeautifulSoup(get_CEO_name, \"html.parser\")\n",
        "            texts = soup.get_text(strip=True)\n",
        "            print(texts)\n",
        "            ceo_name = texts.split(\"is\")[0].strip()\n",
        "            print(f\"CEO name v1: {ceo_name}\")\n",
        "            print(len(ceo_name))\n",
        "            if len(ceo_name) < 25:\n",
        "                # print(ceo_name)\n",
        "                return ceo_name\n",
        "            else:\n",
        "                texts = texts.split(\" \")\n",
        "                for text in texts:\n",
        "                    if text.istitle():\n",
        "                        n.append(text)\n",
        "                        print(f\"Print value ceo: {text}\")\n",
        "                        print(n)\n",
        "                        # print(len(n))\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            if len(n) > 0:\n",
        "                ceo_name = \" \".join(n)\n",
        "\n",
        "            print(ceo_name)\n",
        "            return ceo_name\n",
        "        except Exception as e:\n",
        "            error_message = f\"Error: str{e}\"\n",
        "            print(f\"Error: Not found element hgKElc\")\n",
        "            return ceo_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF6wFaMUMKJz"
      },
      "outputs": [],
      "source": [
        "# @title Search Linkedin\n",
        "def search_linkedin_profile():\n",
        "    # driver = web_driver()\n",
        "    input_file_path = upload_file()\n",
        "\n",
        "    try:\n",
        "        # Export the data to a new Excel file\n",
        "        input_workbook = load_workbook(input_file_path, read_only=True)\n",
        "        input_sheet = input_workbook.active\n",
        "        driver.get(\"https://www.google.com/search?&hl=en\")\n",
        "        # Create a list to store data in JSON format\n",
        "        json_data = []\n",
        "        json_file_path = \"linkedin_data.json\"\n",
        "\n",
        "        for index, row in enumerate(input_sheet.iter_rows(min_row=2, values_only=True)):\n",
        "            if any(cell_value is not None and cell_value != \"\" for cell_value in row):\n",
        "                print(f\"{index}\")\n",
        "                ceo_name = row[0] if row[0] else \"\"\n",
        "                company_name = row[1] if row[1] else \"\"\n",
        "                keywords = row[2] if row[2] else \"\"\n",
        "                ceo_name = \"\"\n",
        "                att = 0\n",
        "\n",
        "                try:\n",
        "                    while not ceo_name:\n",
        "                        att +=1\n",
        "                        if att == 3:\n",
        "                            break\n",
        "                        query = f\"{keywords} of {company_name} \"\n",
        "                        print(query)\n",
        "                        query = query.replace(\" \", \"%20\")\n",
        "                        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                        print(search_url)\n",
        "                        driver.get(search_url)\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        # Scroll_Pages_infinite_loading()\n",
        "                        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "                        text = soup.get_text()\n",
        "                        element = driver.find_element(\n",
        "                            By.CSS_SELECTOR, \"div.eqAnXb > div#search\"\n",
        "                        )\n",
        "                        element_html = element.get_attribute(\"outerHTML\")\n",
        "                        soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                        text = soup.get_text(strip=True)\n",
        "                        org = f\"The {keywords} of Company A is B\"\n",
        "                        check_ = \"\"\n",
        "                        for provider in providers:\n",
        "                            check_ = \"\"\n",
        "                            try:\n",
        "                                response = g4f.ChatCompletion.create(\n",
        "                                    model=\"gpt-3.5-turbo\",\n",
        "                                    provider=provider,\n",
        "                                    messages=[\n",
        "                                        {\n",
        "                                            \"role\": \"user\",\n",
        "                                            \"content\": f\"I will give you a some text about a {keywords} of a company.This is the text :BEGIN: {text[:500]} :END. According the given text, your answer should be just the real {keywords} name,  your answer format answer will be :{org}, no more explain. \",\n",
        "                                        }\n",
        "                                    ],\n",
        "                                )\n",
        "                                for message in response:\n",
        "                                    check_ += message\n",
        "                                print(check_)\n",
        "                                if (\n",
        "                                        \"not\" in check_\n",
        "                                        or not check_\n",
        "                                        or len(check_.split()) > 60\n",
        "                                        or \"is\" not in check_\n",
        "                                ):\n",
        "                                    continue\n",
        "                                else:\n",
        "                                    break\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "                        ceo_name = find_CEO_name()\n",
        "                        print(ceo_name)\n",
        "                    if len(ceo_name) == 0:\n",
        "                        ceo_name = f\"{keywords} not found\"\n",
        "                        json_data.append(\n",
        "                            {\n",
        "                                \"Name\": ceo_name,\n",
        "                                \"Company Name\": company_name,\n",
        "                                \"Keywords\": keywords,\n",
        "                                \"LinkedIn Profile\": \"No LinkedIn links found.\",\n",
        "                            }\n",
        "                        )\n",
        "                    else:\n",
        "                        if att == 3:\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": \"\",\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"LinkedIn Profile\": \"No LinkedIn links found.\",\n",
        "                                }\n",
        "                            )\n",
        "                            break\n",
        "                        query = f\"Linkedin {ceo_name} {keywords} of {company_name}\"\n",
        "                        query = query.replace(\" \", \"%20\")\n",
        "                        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                        driver.get(search_url)\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
        "                        linkedin_links = []\n",
        "                        name_linkedin = []\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        element = driver.find_element(\n",
        "                            By.CSS_SELECTOR, \"div.eqAnXb > div#search\"\n",
        "                        )\n",
        "\n",
        "                        # Get the HTML content of the element\n",
        "                        element_html = element.get_attribute(\"outerHTML\")\n",
        "                        soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                        for link, name in zip(\n",
        "                                [\n",
        "                                    str(tag.get(\"href\")).split(\"?\")[0]\n",
        "                                    for tag in soup.find_all(\"a\")\n",
        "                                ],\n",
        "                                [tag.text for tag in soup.find_all(\"h3\")],\n",
        "                        ):\n",
        "                            # Or get the HTML of the element\n",
        "                            if (\n",
        "                                    \"linkedin.com\" in link\n",
        "                                    and \"/status/\" not in link\n",
        "                                    and \"/post/\" not in link\n",
        "                                    and \"/posts/\" not in link\n",
        "                                    and \"post\" not in link\n",
        "                                    and \"posts\" not in link\n",
        "                                    and \"story\" not in link\n",
        "                                    and \"news\" not in link\n",
        "                                    and \"job\" not in link\n",
        "                                    and \"today\" not in link\n",
        "                                    and \"pulse\" not in link\n",
        "                                    and \"company\" not in link\n",
        "                                    and \"text\" not in link\n",
        "                                    and \"translate\" not in link\n",
        "                                    and \"login\" not in link\n",
        "                                    and \"search\" not in link\n",
        "                                    # and any(\n",
        "                                    #     word.lower() in name.lower()\n",
        "                                    #     for word in ceo_name.split()\n",
        "                                    # )\n",
        "                            ):\n",
        "                                print(link, name)\n",
        "                                linkedin_links.append(link)\n",
        "                                # ceo_name = name.split(\"-\")[0].strip()\n",
        "                                name_linkedin.append(name)\n",
        "                                break\n",
        "                        if (\n",
        "                                not linkedin_links\n",
        "                                or \"posts\" in linkedin_links[-1]\n",
        "                                or \"/posts/\" in linkedin_links[-1]\n",
        "                        ):\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": ceo_name,\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"LinkedIn Profile\": \"No LinkedIn links found.\",\n",
        "                                }\n",
        "                            )\n",
        "                        else:\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": ceo_name,\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"LinkedIn Profile\": linkedin_links[-1],\n",
        "                                }\n",
        "                            )\n",
        "                    with open(json_file_path, \"w\") as json_file:\n",
        "                        json.dump(json_data, json_file, indent=2)\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_message = f\"Error: {str(e)}\"\n",
        "                    print(f\"Error: {str(error_message)}\")\n",
        "        driver.quit()\n",
        "\n",
        "        # Save JSON data to Excel\n",
        "        excel_output_file_path = \"output_linkedin.xlsx\"\n",
        "        output_workbook = Workbook()\n",
        "        output_sheet = output_workbook.active\n",
        "        output_sheet.append(\n",
        "            [\n",
        "                \"Name\",\n",
        "                \"Company Name\",\n",
        "                \"Keywords\",\n",
        "                \"LinkedIn Profile\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for entry in json_data:\n",
        "            output_sheet.append(\n",
        "                [\n",
        "                    entry[\"Name\"],\n",
        "                    entry[\"Company Name\"],\n",
        "                    entry[\"Keywords\"],\n",
        "                    entry[\"LinkedIn Profile\"],\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        output_workbook.save(excel_output_file_path)\n",
        "\n",
        "        files.download(excel_output_file_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error: {str(e)}\"\n",
        "        print(error_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1-TXiJpLAW0"
      },
      "outputs": [],
      "source": [
        "# @title Search Facebook\n",
        "def search_facebook_profile():\n",
        "    input_file_path = upload_file()\n",
        "\n",
        "    try:\n",
        "        # Export the data to a new Excel file\n",
        "        input_workbook = load_workbook(input_file_path, read_only=True)\n",
        "        input_sheet = input_workbook.active\n",
        "        driver.get(\"https://www.google.com/search?&hl=en\")\n",
        "        # Create a list to store data in JSON format\n",
        "        json_data = []\n",
        "        json_file_path = \"facebook_data.json\"\n",
        "\n",
        "        for index, row in enumerate(input_sheet.iter_rows(min_row=2, values_only=True)):\n",
        "            if any(cell_value is not None and cell_value != \"\" for cell_value in row):\n",
        "                print(f\"{index}\")\n",
        "                ceo_name = row[0] if row[0] else \"\"\n",
        "                company_name = row[1] if row[1] else \"\"\n",
        "                keywords = row[2] if row[2] else \"\"\n",
        "                ceo_name = \"\"\n",
        "                att = 0\n",
        "\n",
        "                try:\n",
        "                    while not ceo_name:\n",
        "                        att += 1\n",
        "                        if att == 3:\n",
        "                            break\n",
        "                        query = f\"{keywords} of {company_name} \"\n",
        "                        print(query)\n",
        "                        query = query.replace(\" \", \"%20\")\n",
        "                        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                        driver.get(search_url)\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        # Scroll_Pages_infinite_loading()\n",
        "                        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "                        text = soup.get_text()\n",
        "                        element = driver.find_element(\n",
        "                            By.CSS_SELECTOR, \"div.eqAnXb > div#search\"\n",
        "                        )\n",
        "                        element_html = element.get_attribute(\"outerHTML\")\n",
        "                        soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                        text = soup.get_text(strip=True)\n",
        "                        org = f\"The {keywords} of Company A is B\"\n",
        "                        check_ = \"\"\n",
        "                        for provider in providers:\n",
        "                            check_ = \"\"\n",
        "                            try:\n",
        "                                response = g4f.ChatCompletion.create(\n",
        "                                    model=\"gpt-3.5-turbo\",\n",
        "                                    provider=provider,\n",
        "                                    messages=[\n",
        "                                        {\n",
        "                                            \"role\": \"user\",\n",
        "                                            \"content\": f\"I will give you a some text about a {keywords} of a company.This is the text :BEGIN: {text[:500]} :END. According the given text, your answer should be just the real {keywords} name,  your answer format answer will be :{org}, no more explain. \",\n",
        "                                        }\n",
        "                                    ],\n",
        "                                )\n",
        "                                for message in response:\n",
        "                                    check_ += message\n",
        "                                print(check_)\n",
        "                                if (\n",
        "                                        \"not\" in check_\n",
        "                                        or not check_\n",
        "                                        or len(check_.split()) > 60\n",
        "                                        or \"is\" not in check_\n",
        "                                ):\n",
        "                                    continue\n",
        "                                else:\n",
        "                                    break\n",
        "                            except:\n",
        "                                continue\n",
        "                        ceo_name = find_CEO_name()\n",
        "                        print(ceo_name)\n",
        "                    if len(ceo_name) == 0:\n",
        "                        ceo_name = f\"{keywords} not found\"\n",
        "                        json_data.append(\n",
        "                            {\n",
        "                                \"Name\": ceo_name,\n",
        "                                \"Company Name\": company_name,\n",
        "                                \"Keywords\": keywords,\n",
        "                                \"Facebook Profile\": \"No Facebook links found.\",\n",
        "                            }\n",
        "                        )\n",
        "                    else:\n",
        "                        if att == 3:\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": \"\",\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"Facebook Profile\": \"No Facebook links found.\",\n",
        "                                }\n",
        "                            )\n",
        "                            break\n",
        "                        query = f\"Facebook {ceo_name} {keywords} of {company_name}\"\n",
        "                        query = query.replace(\" \", \"%20\")\n",
        "                        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                        driver.get(search_url)\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
        "                        facebook_links = []\n",
        "                        name_facebook = []\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        element = driver.find_element(\n",
        "                            By.CSS_SELECTOR, \"div.eqAnXb > div#search\"\n",
        "                        )\n",
        "\n",
        "                        # Get the HTML content of the element\n",
        "                        element_html = element.get_attribute(\"outerHTML\")\n",
        "                        soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                        for link, name in zip(\n",
        "                                [\n",
        "                                    str(tag.get(\"href\")).split(\"?\")[0]\n",
        "                                    for tag in soup.find_all(\"a\")\n",
        "                                ],\n",
        "                                [tag.text for tag in soup.find_all(\"h3\")],\n",
        "                        ):\n",
        "                            # Or get the HTML of the element\n",
        "                            if (\n",
        "                                    \"facebook.com\" in link\n",
        "                                    and \"/status/\" not in link\n",
        "                                    and \"/post/\" not in link\n",
        "                                    and \"/posts/\" not in link\n",
        "                                    and \"post\" not in link\n",
        "                                    and \"posts\" not in link\n",
        "                                    and \"story\" not in link\n",
        "                                    and \"news\" not in link\n",
        "                                    and \"job\" not in link\n",
        "                                    and \"today\" not in link\n",
        "                                    and \"pulse\" not in link\n",
        "                                    and \"company\" not in link\n",
        "                                    and \"text\" not in link\n",
        "                                    and \"translate\" not in link\n",
        "                                    and \"login\" not in link\n",
        "                                    and \"search\" not in link\n",
        "                                    # and any(\n",
        "                                    #     word.lower() in name.lower()\n",
        "                                    #     for word in ceo_name.split()\n",
        "                                    # )\n",
        "                            ):\n",
        "                                print(link, name)\n",
        "                                facebook_links.append(link)\n",
        "                                # ceo_name = name.split(\"-\")[0].strip()\n",
        "                                name_facebook.append(name)\n",
        "                                break\n",
        "                        if (\n",
        "                                not facebook_links\n",
        "                                or \"posts\" in facebook_links[-1]\n",
        "                                or \"/posts/\" in facebook_links[-1]\n",
        "                        ):\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": ceo_name,\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"Facebook Profile\": \"No Facebook links found.\",\n",
        "                                }\n",
        "                            )\n",
        "                        else:\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": ceo_name,\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"Facebook Profile\": facebook_links[-1],\n",
        "                                }\n",
        "                            )\n",
        "\n",
        "                    with open(json_file_path, \"w\") as json_file:\n",
        "                        json.dump(json_data, json_file, indent=2)\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_message = f\"Error: {str(e)}\"\n",
        "                    print(f\"Error: {str(error_message)}\")\n",
        "        driver.quit()\n",
        "\n",
        "        # Save JSON data to Excel\n",
        "        excel_output_file_path = \"output_facebook.xlsx\"\n",
        "        output_workbook = Workbook()\n",
        "        output_sheet = output_workbook.active\n",
        "        output_sheet.append(\n",
        "            [\n",
        "                \"Name\",\n",
        "                \"Company Name\",\n",
        "                \"Keywords\",\n",
        "                \"Facebook Profile\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for entry in json_data:\n",
        "            output_sheet.append(\n",
        "                [\n",
        "                    entry[\"Name\"],\n",
        "                    entry[\"Company Name\"],\n",
        "                    entry[\"Keywords\"],\n",
        "                    entry[\"Facebook Profile\"],\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        output_workbook.save(excel_output_file_path)\n",
        "\n",
        "        files.download(excel_output_file_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error: {str(e)}\"\n",
        "        print(error_message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iZG9DqdYjkW"
      },
      "outputs": [],
      "source": [
        "# @title Search Twitter\n",
        "def search_twitter_profile():\n",
        "    # driver = web_driver()\n",
        "    input_file_path = upload_file()\n",
        "\n",
        "    try:\n",
        "        # Export the data to a new Excel file\n",
        "        input_workbook = load_workbook(input_file_path, read_only=True)\n",
        "        input_sheet = input_workbook.active\n",
        "        driver.get(\"https://www.google.com/search?&hl=en\")\n",
        "        # Create a list to store data in JSON format\n",
        "        json_file_path = \"twitter_data.json\"\n",
        "        json_data = []\n",
        "\n",
        "        for index, row in enumerate(input_sheet.iter_rows(min_row=2, values_only=True)):\n",
        "            if any(cell_value is not None and cell_value != \"\" for cell_value in row):\n",
        "                print(f\"{index}\")\n",
        "                ceo_name = row[0] if row[0] else \"\"\n",
        "                company_name = row[1] if row[1] else \"\"\n",
        "                keywords = row[2] if row[2] else \"\"\n",
        "                ceo_name = \"\"\n",
        "                att = 0\n",
        "                try:\n",
        "                    while not ceo_name:\n",
        "                        att += 1\n",
        "                        # Exit While\n",
        "                        if att == 3:\n",
        "                            break\n",
        "                        query = f\"{keywords} of {company_name} \"\n",
        "                        print(query)\n",
        "                        query = query.replace(\" \", \"%20\")\n",
        "                        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                        driver.get(search_url)\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "                        text = soup.get_text()\n",
        "                        element = driver.find_element(\n",
        "                            By.CSS_SELECTOR, \"div.eqAnXb > div#search\"\n",
        "                        )\n",
        "                        element_html = element.get_attribute(\"outerHTML\")\n",
        "                        soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                        text = soup.get_text(strip=True)\n",
        "                        org = f\"The {keywords} of Company A is B\"\n",
        "                        check_ = \"\"\n",
        "                        for provider in providers:\n",
        "                            check_ = \"\"\n",
        "                            try:\n",
        "                                content = f\"I will give you a some text about a {keywords} of a company. This is the text :BEGIN: {text[:500]} :END. According the given text, your answer should be just the real {keywords} name,  your answer format answer will be :{org}, no more explain. \"\n",
        "                                response = g4f.ChatCompletion.create(\n",
        "                                    model=\"gpt-3.5-turbo\",\n",
        "                                    provider=provider,\n",
        "                                    messages=[\n",
        "                                        {\n",
        "                                            \"role\": \"user\",\n",
        "                                            \"content\": content,\n",
        "                                        }\n",
        "                                    ],\n",
        "                                )\n",
        "                                for message in response:\n",
        "                                    check_ += message\n",
        "                                print(check_)\n",
        "                                if (\n",
        "                                        \"not\" in check_\n",
        "                                        or not check_\n",
        "                                        or len(check_.split()) > 60\n",
        "                                        or \"is\" not in check_\n",
        "                                ):\n",
        "                                    continue\n",
        "                                else:\n",
        "                                    break\n",
        "                            except:\n",
        "                                continue\n",
        "                        ceo_name = find_CEO_name()\n",
        "                        print(ceo_name)\n",
        "\n",
        "                    if len(ceo_name) == 0:\n",
        "                        ceo_name = f\"{keywords} not found\"\n",
        "                        json_data.append(\n",
        "                            {\n",
        "                                \"Name\": ceo_name,\n",
        "                                \"Company Name\": company_name,\n",
        "                                \"Keywords\": keywords,\n",
        "                                \"Twitter Profile\": \"No Twitter links found.\"\n",
        "                            }\n",
        "                        )\n",
        "                    else:\n",
        "                        if att == 3:\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": ceo_name,\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"Twitter Profile\": \"No Twitter links found.\"\n",
        "                                }\n",
        "                            )\n",
        "                            break\n",
        "                        query = f\"twitter {ceo_name} {keywords} of {company_name}\"\n",
        "                        query = query.replace(\" \", \"%20\")\n",
        "                        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                        driver.get(search_url)\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
        "                        twitter_links = []\n",
        "                        name_twitter = []\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        element = driver.find_element(\n",
        "                            By.CSS_SELECTOR, \"div.eqAnXb > div#search\"\n",
        "                        )\n",
        "\n",
        "                        # Get the HTML content of the element\n",
        "                        element_html = element.get_attribute(\"outerHTML\")\n",
        "\n",
        "                        soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                        # Extract href attributes\n",
        "\n",
        "                        # Extract href attributes\n",
        "                        # Extract the text or HTML of the element\n",
        "                        for link, name in zip(\n",
        "                                [\n",
        "                                    str(tag.get(\"href\")).split(\"?\")[0]\n",
        "                                    for tag in soup.find_all(\"a\")\n",
        "                                ],\n",
        "                                [tag.text for tag in soup.find_all(\"h3\")],\n",
        "                        ):\n",
        "                            # Or get the HTML of the element\n",
        "                            if (\n",
        "                                    \"twitter.com\" in link\n",
        "                                    and \"/status/\" not in link\n",
        "                                    and \"/post/\" not in link\n",
        "                                    and \"/posts/\" not in link\n",
        "                                    and \"post\" not in link\n",
        "                                    and \"posts\" not in link\n",
        "                                    and \"story\" not in link\n",
        "                                    and \"news\" not in link\n",
        "                                    and \"job\" not in link\n",
        "                                    and \"today\" not in link\n",
        "                                    and \"pulse\" not in link\n",
        "                                    and \"company\" not in link\n",
        "                                    and \"text\" not in link\n",
        "                                    and \"translate\" not in link\n",
        "                                    and \"login\" not in link\n",
        "                                    and \"search\" not in link\n",
        "                                    # and any(\n",
        "                                    #     word.lower() in name.lower()\n",
        "                                    #     for word in ceo_name.split()\n",
        "                                    # )\n",
        "                            ):\n",
        "                                print(link, name)\n",
        "                                twitter_links.append(link)\n",
        "                                if name.find(\"(\") > 0:\n",
        "                                  ceo_name = name.split(\"(\")[0].strip()\n",
        "                                name_twitter.append(name)\n",
        "                                break\n",
        "                        if (\n",
        "                                not twitter_links\n",
        "                                or \"posts\" in twitter_links[-1]\n",
        "                                or \"/posts/\" in twitter_links[-1]\n",
        "                        ):\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": ceo_name,\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"Twitter Profile\": \"No twitter links found.\",\n",
        "                                }\n",
        "                            )\n",
        "                        else:\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": ceo_name,\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"Twitter Profile\": twitter_links[-1],\n",
        "                                }\n",
        "                            )\n",
        "                    with open(json_file_path, \"w\") as json_file:\n",
        "                        json.dump(json_data, json_file, indent=2)\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_message = f\"Error: {str(e)}\"\n",
        "                    print(error_message)\n",
        "\n",
        "        driver.quit()\n",
        "\n",
        "        # Save JSON data to Excel\n",
        "        excel_output_file_path = \"output_twitter.xlsx\"\n",
        "        output_workbook = Workbook()\n",
        "        output_sheet = output_workbook.active\n",
        "        output_sheet.append(\n",
        "            [\n",
        "                \"Name\",\n",
        "                \"Company Name\",\n",
        "                \"Keywords\",\n",
        "                \"Twitter Profile\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for entry in json_data:\n",
        "            output_sheet.append(\n",
        "                [\n",
        "                    entry[\"Name\"],\n",
        "                    entry[\"Company Name\"],\n",
        "                    entry[\"Keywords\"],\n",
        "                    entry[\"Twitter Profile\"],\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        output_workbook.save(excel_output_file_path)\n",
        "        files.download(excel_output_file_path)\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error: {str(e)}\"\n",
        "        # status_label.config(text=error_message)\n",
        "        print(error_message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aciaoJPr5LD9"
      },
      "outputs": [],
      "source": [
        "# @title Search Email\n",
        "def search_email():\n",
        "    input_file_path = upload_file()\n",
        "\n",
        "    try:\n",
        "        # Export the data to a new Excel file\n",
        "        input_workbook = load_workbook(input_file_path, read_only=True)\n",
        "        input_sheet = input_workbook.active\n",
        "        driver.get(\"https://www.google.com/search?&hl=en\")\n",
        "        json_data = []\n",
        "        list_email = []\n",
        "        json_file_path = \"email.json\"\n",
        "\n",
        "        for index, row in enumerate(input_sheet.iter_rows(min_row=2, values_only=True)):\n",
        "            if any(cell_value is not None and cell_value != \"\" for cell_value in row):\n",
        "                print(f\"{index}\")\n",
        "                ceo_name = row[0] if row[0] else \"\"\n",
        "                company_name = row[1] if row[1] else \"\"\n",
        "                keywords = row[2] if row[2] else \"\"\n",
        "                ceo_name = \"\"\n",
        "                att = 0\n",
        "                try:\n",
        "                    while not ceo_name:\n",
        "                        att =+ 1\n",
        "                        if att == 3:\n",
        "                            break\n",
        "                        query = f\"{keywords} of {company_name} \"\n",
        "                        print(query)\n",
        "                        query = query.replace(\" \", \"%20\")\n",
        "                        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                        driver.get(search_url)\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        # Scroll_Pages_infinite_loading()\n",
        "                        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "                        text = soup.get_text()\n",
        "                        element = driver.find_element(\n",
        "                            By.CSS_SELECTOR, \"div.eqAnXb > div#search\"\n",
        "                        )\n",
        "                        element_html = element.get_attribute(\"outerHTML\")\n",
        "                        soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                        text = soup.get_text(strip=True)\n",
        "                        org = f\"The {keywords} of Company A is B\"\n",
        "                        check_ = \"\"\n",
        "                        for provider in providers:\n",
        "                            check_ = \"\"\n",
        "                            try:\n",
        "                                response = g4f.ChatCompletion.create(\n",
        "                                    model=\"gpt-3.5-turbo\",\n",
        "                                    provider=provider,\n",
        "                                    messages=[\n",
        "                                        {\n",
        "                                            \"role\": \"user\",\n",
        "                                            \"content\": f\"I will give you a some text about a {keywords} of a company.This is the text :BEGIN: {text[:500]} :END. According the given text, your answer should be just the real {keywords} name,  your answer format answer will be :{org}, no more explain. \",\n",
        "                                        }\n",
        "                                    ],\n",
        "                                )\n",
        "                                for message in response:\n",
        "                                    check_ += message\n",
        "                                print(check_)\n",
        "                                if (\n",
        "                                        \"not\" in check_\n",
        "                                        or not check_\n",
        "                                        or len(check_.split()) > 60\n",
        "                                        or \"is\" not in check_\n",
        "                                ):\n",
        "                                    continue\n",
        "                                else:\n",
        "                                    break\n",
        "                            except:\n",
        "                                continue\n",
        "                        ceo_name = find_CEO_name()\n",
        "                        print(ceo_name)\n",
        "                    if len(ceo_name) == 0:\n",
        "                        ceo_name = f\"{keywords} not found\"\n",
        "                        json_data.append(\n",
        "                            {\n",
        "                                \"Name\": ceo_name,\n",
        "                                \"Company Name\": company_name,\n",
        "                                \"Keywords\": keywords,\n",
        "                                \"Email\": \"No Email links found.\",\n",
        "                            }\n",
        "                        )\n",
        "                    else:\n",
        "                        if att == 3:\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": \"\",\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"Email\": \"No Email links found.\",\n",
        "                                }\n",
        "                            )\n",
        "                            break\n",
        "                        query = f\"{company_name} website\"\n",
        "                        query = query.replace(\" \", \"%20\")\n",
        "                        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                        driver.get(search_url)\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "                        website = soup.find('div', class_=\"yuRUbf\")\n",
        "                        if type(website) != None:\n",
        "                            link = website.a.get('href')\n",
        "                            description = str(soup.find('div', class_=\"NJo7tc Z26q7c uUuwM\"))\n",
        "                            # if df.iloc[i,0] in description:\n",
        "                            #     print(link)\n",
        "                            driver.find_element(By.NAME, \"q\").clear()\n",
        "                            driver.find_element(By.NAME, \"q\").send_keys(link + \" contact\" + Keys.ENTER)\n",
        "                            time.sleep(random.randint(3, 9))  # Let the user actually see something!\n",
        "                            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "                            website = soup.find('div', class_=\"yuRUbf\")\n",
        "                            if type(website) != None:\n",
        "                                link_contact = website.a.get('href')\n",
        "                            ### read url from input\n",
        "                            # df.iloc[i, 1] = link_contact\n",
        "                            original_url = link_contact\n",
        "                            # to save urls to be scraped\n",
        "                            unscraped = deque([original_url])\n",
        "                            ### to save scraped urls\n",
        "                            scraped = set()\n",
        "                            ### to save fetched emails\n",
        "                            emails = set()\n",
        "                            email_pattern = r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,4}\"\n",
        "                            html = driver.page_source\n",
        "\n",
        "                            while len(unscraped):\n",
        "                                url = unscraped.popleft()\n",
        "                                scraped.add(url)\n",
        "                                parts = urlsplit(url)\n",
        "                                base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
        "                                if '/' in parts.path:\n",
        "                                    path = url[:url.rfind('/') + 1]\n",
        "                                else:\n",
        "                                    path = url\n",
        "                                print(\"Crawling URL %s\" % url)\n",
        "                                try:\n",
        "                                    response = requests.get(url)\n",
        "                                    response.raise_for_status()\n",
        "                                except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
        "                                    continue\n",
        "                                new_emails = set(\n",
        "                                    re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,4}\", response.text,\n",
        "                                               re.I))\n",
        "                                emails.update(new_emails)\n",
        "                                soup = BeautifulSoup(response.text, 'lxml')\n",
        "                                for anchor in soup.find_all(\"a\"):\n",
        "                                    if \"href\" in anchor.attrs:\n",
        "                                        link = anchor.attrs[\"href\"]\n",
        "                                    else:\n",
        "                                        link = ''\n",
        "                                        if link.startswith('/'):\n",
        "                                            link = base_url + link\n",
        "                                        elif not link.startswith('http'):\n",
        "                                            link = path + link\n",
        "                            isEmpty = (len(emails) == 0)  # Check email detected or Not\n",
        "                            if isEmpty:\n",
        "                                email_pattern = r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,4}\"\n",
        "                                html = driver.page_source\n",
        "                                emails = re.findall(email_pattern, html)\n",
        "                                print(emails)\n",
        "                                list_email.append(str(emails))\n",
        "\n",
        "                                json_data.append(\n",
        "                                    {\n",
        "                                        \"Name\": ceo_name,\n",
        "                                        \"Company Name\": company_name,\n",
        "                                        \"Keywords\": keywords,\n",
        "                                        \"Email\": list_email[-1],\n",
        "                                    }\n",
        "                                )\n",
        "                            else:\n",
        "                                print(emails)\n",
        "                                list_email.append(str(emails))\n",
        "\n",
        "                                json_data.append(\n",
        "                                    {\n",
        "                                        \"Name\": ceo_name,\n",
        "                                        \"Company Name\": company_name,\n",
        "                                        \"Keywords\": keywords,\n",
        "                                        \"Email\": list_email[-1],\n",
        "                                    }\n",
        "                                )\n",
        "                        driver.delete_all_cookies\n",
        "\n",
        "                    with open(json_file_path, \"w\") as json_file:\n",
        "                        json.dump(json_data, json_file, indent=2)\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_message = f\"Error: {str(e)}\"\n",
        "                    print(f\"Error: {str(error_message)}\")\n",
        "        driver.quit()\n",
        "\n",
        "        # Save JSON data to Excel\n",
        "        excel_output_file_path = \"output_email.xlsx\"\n",
        "        output_workbook = Workbook()\n",
        "        output_sheet = output_workbook.active\n",
        "        output_sheet.append(\n",
        "            [\n",
        "                \"Name\",\n",
        "                \"Company Name\",\n",
        "                \"Keywords\",\n",
        "                \"Email\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for entry in json_data:\n",
        "            output_sheet.append(\n",
        "                [\n",
        "                    entry[\"Name\"],\n",
        "                    entry[\"Company Name\"],\n",
        "                    entry[\"Keywords\"],\n",
        "                    entry[\"Email\"],\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        output_workbook.save(excel_output_file_path)\n",
        "        files.download(excel_output_file_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error: {str(e)}\"\n",
        "        print(error_message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzGl6nK7yAla"
      },
      "outputs": [],
      "source": [
        "# @title Find All\n",
        "def search_all():\n",
        "    input_file_path = upload_file()\n",
        "\n",
        "    try:\n",
        "        # Export the data to a new Excel file\n",
        "        input_workbook = load_workbook(input_file_path, read_only=True)\n",
        "        input_sheet = input_workbook.active\n",
        "        driver.get(\"https://www.google.com/search?&hl=en\")\n",
        "        # Create a list to store data in JSON format\n",
        "        json_data = []\n",
        "        list_email = []\n",
        "        json_file_path = \"all.json\"\n",
        "\n",
        "        for index, row in enumerate(input_sheet.iter_rows(min_row=2, values_only=True)):\n",
        "            if any(cell_value is not None and cell_value != \"\" for cell_value in row):\n",
        "                print(f\"{index}\")\n",
        "                ceo_name = row[0] if row[0] else \"\"\n",
        "                company_name = row[1] if row[1] else \"\"\n",
        "                keywords = row[2] if row[2] else \"\"\n",
        "                ceo_name = \"\"\n",
        "                att = 0\n",
        "                try:\n",
        "                    while not ceo_name:\n",
        "                        att += 1\n",
        "                        if att == 3:\n",
        "                            break\n",
        "                        query = f\"{keywords} of {company_name} \"\n",
        "                        print(query)\n",
        "                        query = query.replace(\" \", \"%20\")\n",
        "                        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                        driver.get(search_url)\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        # Scroll_Pages_infinite_loading()\n",
        "                        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "                        text = soup.get_text()\n",
        "                        element = driver.find_element(\n",
        "                            By.CSS_SELECTOR, \"div.eqAnXb > div#search\"\n",
        "                        )\n",
        "                        element_html = element.get_attribute(\"outerHTML\")\n",
        "                        soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                        text = soup.get_text(strip=True)\n",
        "                        org = f\"The {keywords} of Company A is B\"\n",
        "                        check_ = \"\"\n",
        "                        for provider in providers:\n",
        "                            check_ = \"\"\n",
        "                            try:\n",
        "                                response = g4f.ChatCompletion.create(\n",
        "                                    model=\"gpt-3.5-turbo\",\n",
        "                                    provider=provider,\n",
        "                                    messages=[\n",
        "                                        {\n",
        "                                            \"role\": \"user\",\n",
        "                                            \"content\": f\"I will give you a some text about a {keywords} of a company.This is the text :BEGIN: {text[:500]} :END. According the given text, your answer should be just the real {keywords} name,  your answer format answer will be :{org}, no more explain. \",\n",
        "                                        }\n",
        "                                    ],\n",
        "                                )\n",
        "                                for message in response:\n",
        "                                    check_ += message\n",
        "                                print(check_)\n",
        "                                if (\n",
        "                                        \"not\" in check_\n",
        "                                        or not check_\n",
        "                                        or len(check_.split()) > 60\n",
        "                                        or \"is\" not in check_\n",
        "                                ):\n",
        "                                    continue\n",
        "                                else:\n",
        "                                    break\n",
        "                            except:\n",
        "                                continue\n",
        "                        ceo_name = find_CEO_name()\n",
        "                        print(ceo_name)\n",
        "                    if len(ceo_name) == 0:\n",
        "                        ceo_name = f\"{keywords} not found\"\n",
        "                        json_data.append(\n",
        "                            {\n",
        "                                \"Name\": ceo_name,\n",
        "                                \"Company Name\": company_name,\n",
        "                                \"Keywords\": keywords,\n",
        "                                \"Email\": \"No Email links found.\",\n",
        "                                \"LinkedIn Profile\": \"No LinkedIn links found.\",\n",
        "                                \"Twitter Profile\": \"No Twitter links found\",\n",
        "                                \"Facebook Profile\": \"No Facebook links found.\",\n",
        "                            }\n",
        "                        )\n",
        "                    else:\n",
        "                        if att == 3:\n",
        "                            json_data.append(\n",
        "                                {\n",
        "                                    \"Name\": \"\",\n",
        "                                    \"Company Name\": company_name,\n",
        "                                    \"Keywords\": keywords,\n",
        "                                    \"Email\": \"No Email links found.\",\n",
        "                                    \"LinkedIn Profile\": \"No LinkedIn links found.\",\n",
        "                                    \"Twitter Profile\": \"No Twitter links found\",\n",
        "                                    \"Facebook Profile\": \"No Facebook links found.\",\n",
        "                                }\n",
        "                            )\n",
        "                            break\n",
        "                        query = f\"{company_name} website\"\n",
        "                        query = query.replace(\" \", \"%20\")\n",
        "                        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                        driver.get(search_url)\n",
        "                        time.sleep(random.randint(3, 9))\n",
        "                        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "                        website = soup.find('div', class_=\"yuRUbf\")\n",
        "                        if type(website) != None:\n",
        "                            link = website.a.get('href')\n",
        "                            description = str(soup.find('div', class_=\"NJo7tc Z26q7c uUuwM\"))\n",
        "                            # if df.iloc[i,0] in description:\n",
        "                            #     print(link)\n",
        "                            driver.find_element(By.NAME, \"q\").clear()\n",
        "                            driver.find_element(By.NAME, \"q\").send_keys(link + \" contact\" + Keys.ENTER)\n",
        "                            time.sleep(random.randint(3, 9))  # Let the user actually see something!\n",
        "                            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "                            website = soup.find('div', class_=\"yuRUbf\")\n",
        "                            if type(website) != None:\n",
        "                                link_contact = website.a.get('href')\n",
        "                            ### read url from input\n",
        "                            # df.iloc[i, 1] = link_contact\n",
        "                            original_url = link_contact\n",
        "                            # to save urls to be scraped\n",
        "                            unscraped = deque([original_url])\n",
        "                            ### to save scraped urls\n",
        "                            scraped = set()\n",
        "                            ### to save fetched emails\n",
        "                            emails = set()\n",
        "                            email_pattern = r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,4}\"\n",
        "                            html = driver.page_source\n",
        "\n",
        "                            while len(unscraped):\n",
        "                                url = unscraped.popleft()\n",
        "                                scraped.add(url)\n",
        "                                parts = urlsplit(url)\n",
        "                                base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
        "                                if '/' in parts.path:\n",
        "                                    path = url[:url.rfind('/') + 1]\n",
        "                                else:\n",
        "                                    path = url\n",
        "                                print(\"Crawling URL %s\" % url)\n",
        "                                try:\n",
        "                                    response = requests.get(url)\n",
        "                                    response.raise_for_status()\n",
        "                                except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
        "                                    continue\n",
        "                                new_emails = set(\n",
        "                                    re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,4}\", response.text,\n",
        "                                               re.I))\n",
        "                                emails.update(new_emails)\n",
        "                                soup = BeautifulSoup(response.text, 'lxml')\n",
        "                                for anchor in soup.find_all(\"a\"):\n",
        "                                    if \"href\" in anchor.attrs:\n",
        "                                        link = anchor.attrs[\"href\"]\n",
        "                                    else:\n",
        "                                        link = ''\n",
        "                                        if link.startswith('/'):\n",
        "                                            link = base_url + link\n",
        "                                        elif not link.startswith('http'):\n",
        "                                            link = path + link\n",
        "                            isEmpty = (len(emails) == 0)  # Check email detected or Not\n",
        "                            if isEmpty:\n",
        "                                email_pattern = r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,4}\"\n",
        "                                html = driver.page_source\n",
        "                                emails = re.findall(email_pattern, html)\n",
        "                                print(emails)\n",
        "                                list_email.append(str(emails))\n",
        "\n",
        "                                json_data.append(\n",
        "                                    {\n",
        "                                        \"Name\": ceo_name,\n",
        "                                        \"Company Name\": company_name,\n",
        "                                        \"Keywords\": keywords,\n",
        "                                        \"Email\": list_email[-1],\n",
        "                                    }\n",
        "                                )\n",
        "                            else:\n",
        "                                print(emails)\n",
        "                                list_email.append(str(emails))\n",
        "\n",
        "                                json_data.append(\n",
        "                                    {\n",
        "                                        \"Name\": ceo_name,\n",
        "                                        \"Company Name\": company_name,\n",
        "                                        \"Keywords\": keywords,\n",
        "                                        \"Email\": list_email[-1],\n",
        "                                    }\n",
        "                                )\n",
        "                        driver.delete_all_cookies\n",
        "\n",
        "                        list_search = [\"Linkedin\", \"Facebook\", \"Twitter\"]\n",
        "                        linkedin_links = []\n",
        "                        name_linkedin = []\n",
        "                        facebook_links = []\n",
        "                        name_facebook = []\n",
        "                        twitter_links = []\n",
        "                        name_twitter = []\n",
        "                        for search in list_search:\n",
        "\n",
        "                            query = f\"{search} {ceo_name} {keywords} of {company_name}\"\n",
        "                            query = query.replace(\" \", \"%20\")\n",
        "                            search_url = f\"https://www.google.com/search?q={query}\"\n",
        "                            driver.get(search_url)\n",
        "                            time.sleep(random.randint(3, 9))\n",
        "                            driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
        "                            if \"Linkedin\" in search:\n",
        "                                # linkedin_links = []\n",
        "                                # name_linkedin = []\n",
        "                                time.sleep(random.randint(3, 9))\n",
        "                                element = driver.find_element(\n",
        "                                    By.CSS_SELECTOR, \"div.eqAnXb > div#search\"\n",
        "                                )\n",
        "\n",
        "                                # Get the HTML content of the element\n",
        "                                element_html = element.get_attribute(\"outerHTML\")\n",
        "                                soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                                for link, name in zip(\n",
        "                                        [\n",
        "                                            str(tag.get(\"href\")).split(\"?\")[0]\n",
        "                                            for tag in soup.find_all(\"a\")\n",
        "                                        ],\n",
        "                                        [tag.text for tag in soup.find_all(\"h3\")],\n",
        "                                ):\n",
        "                                    # Or get the HTML of the element\n",
        "                                    if (\n",
        "                                            \"linkedin.com\" in link\n",
        "                                            and \"/status/\" not in link\n",
        "                                            and \"/post/\" not in link\n",
        "                                            and \"/posts/\" not in link\n",
        "                                            and \"post\" not in link\n",
        "                                            and \"posts\" not in link\n",
        "                                            and \"story\" not in link\n",
        "                                            and \"news\" not in link\n",
        "                                            and \"job\" not in link\n",
        "                                            and \"today\" not in link\n",
        "                                            and \"pulse\" not in link\n",
        "                                            and \"company\" not in link\n",
        "                                            and \"text\" not in link\n",
        "                                            and \"translate\" not in link\n",
        "                                            and \"login\" not in link\n",
        "                                            and \"search\" not in link\n",
        "                                            # and any(\n",
        "                                            #     word.lower() in name.lower()\n",
        "                                            #     for word in ceo_name.split()\n",
        "                                            # )\n",
        "                                    ):\n",
        "                                        print(link, name)\n",
        "                                        linkedin_links.append(link)\n",
        "                                        # ceo_name = name.split(\"-\")[0].strip()\n",
        "                                        name_linkedin.append(name)\n",
        "                                        print(\"check1\")\n",
        "                                        break\n",
        "                                if (\n",
        "                                        not linkedin_links\n",
        "                                        or \"posts\" in linkedin_links[-1]\n",
        "                                        or \"/posts/\" in linkedin_links[-1]\n",
        "                                ):\n",
        "                                    json_data[-1].update(\n",
        "                                        {\n",
        "                                            \"LinkedIn Profile\": \"No LinkedIn links found.\",\n",
        "                                        }\n",
        "                                    )\n",
        "\n",
        "                                else:\n",
        "                                    json_data[-1].update(\n",
        "                                        {\n",
        "                                            \"LinkedIn Profile\": linkedin_links[-1],\n",
        "                                        }\n",
        "                                    )\n",
        "                            elif \"Facebook\" in search:\n",
        "                                # facebook_links = []\n",
        "                                # name_facebook = []\n",
        "                                time.sleep(random.randint(3, 9))\n",
        "                                element = driver.find_element(\n",
        "                                    By.CSS_SELECTOR, \"div.eqAnXb > div#search\"\n",
        "                                )\n",
        "\n",
        "                                # Get the HTML content of the element\n",
        "                                element_html = element.get_attribute(\"outerHTML\")\n",
        "                                soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                                for link, name in zip(\n",
        "                                        [\n",
        "                                            str(tag.get(\"href\")).split(\"?\")[0]\n",
        "                                            for tag in soup.find_all(\"a\")\n",
        "                                        ],\n",
        "                                        [tag.text for tag in soup.find_all(\"h3\")],\n",
        "                                ):\n",
        "                                    # Or get the HTML of the element\n",
        "                                    if (\n",
        "                                            \"facebook.com\" in link\n",
        "                                            and \"/status/\" not in link\n",
        "                                            and \"/post/\" not in link\n",
        "                                            and \"/posts/\" not in link\n",
        "                                            and \"post\" not in link\n",
        "                                            and \"posts\" not in link\n",
        "                                            and \"story\" not in link\n",
        "                                            and \"news\" not in link\n",
        "                                            and \"job\" not in link\n",
        "                                            and \"today\" not in link\n",
        "                                            and \"pulse\" not in link\n",
        "                                            and \"company\" not in link\n",
        "                                            and \"text\" not in link\n",
        "                                            and \"translate\" not in link\n",
        "                                            and \"login\" not in link\n",
        "                                            and \"search\" not in link\n",
        "                                            # and any(\n",
        "                                            #     word.lower() in name.lower()\n",
        "                                            #     for word in ceo_name.split()\n",
        "                                            # )\n",
        "                                    ):\n",
        "                                        print(link, name)\n",
        "                                        facebook_links.append(link)\n",
        "                                        # ceo_name = name.split(\"-\")[0].strip()\n",
        "                                        name_facebook.append(name)\n",
        "                                        print(\"check2\")\n",
        "                                        break\n",
        "                                if (\n",
        "                                        not facebook_links\n",
        "                                        or \"posts\" in facebook_links[-1]\n",
        "                                        or \"/posts/\" in facebook_links[-1]\n",
        "                                ):\n",
        "                                    json_data[-1].update(\n",
        "                                        {\n",
        "                                            \"Facebook Profile\": \"No Facebook links found.\",\n",
        "                                        }\n",
        "                                    )\n",
        "                                else:\n",
        "                                    json_data[-1].update(\n",
        "                                        {\n",
        "                                            \"Facebook Profile\": facebook_links[-1],\n",
        "                                        }\n",
        "                                    )\n",
        "                            else:\n",
        "                                time.sleep(random.randint(3, 9))\n",
        "                                element = driver.find_element(\n",
        "                                    By.CSS_SELECTOR, \"div.eqAnXb > div#search > div > div\"\n",
        "                                )\n",
        "\n",
        "                                # Get the HTML content of the element\n",
        "                                element_html = element.get_attribute(\"outerHTML\")\n",
        "\n",
        "                                soup = BeautifulSoup(element_html, \"html.parser\")\n",
        "                                # Extract href attributes\n",
        "\n",
        "                                # Extract href attributes\n",
        "                                # Extract the text or HTML of the element\n",
        "                                for link, name in zip(\n",
        "                                        [\n",
        "                                            str(tag.get(\"href\")).split(\"?\")[0]\n",
        "                                            for tag in soup.find_all(\"a\")\n",
        "                                        ],\n",
        "                                        [tag.text for tag in soup.find_all(\"h3\")],\n",
        "                                ):\n",
        "                                    # Or get the HTML of the element\n",
        "                                    if (\n",
        "                                            \"twitter.com\" in link\n",
        "                                            and \"/status/\" not in link\n",
        "                                            and \"/post/\" not in link\n",
        "                                            and \"/posts/\" not in link\n",
        "                                            and \"post\" not in link\n",
        "                                            and \"posts\" not in link\n",
        "                                            and \"story\" not in link\n",
        "                                            and \"news\" not in link\n",
        "                                            and \"job\" not in link\n",
        "                                            and \"today\" not in link\n",
        "                                            and \"pulse\" not in link\n",
        "                                            and \"company\" not in link\n",
        "                                            and \"text\" not in link\n",
        "                                            and \"translate\" not in link\n",
        "                                            and \"login\" not in link\n",
        "                                            and \"search\" not in link\n",
        "                                            # and any(\n",
        "                                            #     word.lower() in name.lower()\n",
        "                                            #     for word in ceo_name.split()\n",
        "                                            # )\n",
        "                                    ):\n",
        "                                        print(link, name)\n",
        "                                        twitter_links.append(link)\n",
        "                                        # ceo_name = name.split(\"-\")[0].strip()\n",
        "                                        name_twitter.append(name)\n",
        "                                        break\n",
        "                                if (\n",
        "                                        not twitter_links\n",
        "                                        or \"posts\" in twitter_links[-1]\n",
        "                                        or \"/posts/\" in twitter_links[-1]\n",
        "                                ):\n",
        "                                    json_data[-1].update(\n",
        "                                        {\n",
        "                                            \"Twitter Profile\": \"No twitter links found.\",\n",
        "                                        }\n",
        "                                    )\n",
        "                                else:\n",
        "                                    json_data[-1].update(\n",
        "                                        {\n",
        "                                            \"Twitter Profile\": twitter_links[-1],\n",
        "                                        }\n",
        "                                    )\n",
        "\n",
        "                    with open(json_file_path, \"w\") as json_file:\n",
        "                        json.dump(json_data, json_file, indent=2)\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_message = f\"Error: {str(e)}\"\n",
        "                    print(f\"Error: {str(error_message)}\")\n",
        "\n",
        "        driver.quit()\n",
        "\n",
        "        # Save JSON data to Excel\n",
        "        excel_output_file_path = \"output_find_all.xlsx\"\n",
        "        output_workbook = Workbook()\n",
        "        output_sheet = output_workbook.active\n",
        "        output_sheet.append(\n",
        "            [\n",
        "                \"Name\",\n",
        "                \"Company Name\",\n",
        "                \"Keywords\",\n",
        "                \"Email\",\n",
        "                \"LinkedIn Profile\",\n",
        "                \"Facebook Profile\",\n",
        "                \"Twitter Profile\"\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for entry in json_data:\n",
        "            output_sheet.append(\n",
        "                [\n",
        "                    entry[\"Name\"],\n",
        "                    entry[\"Company Name\"],\n",
        "                    entry[\"Keywords\"],\n",
        "                    entry[\"Email\"],\n",
        "                    entry[\"LinkedIn Profile\"],\n",
        "                    entry[\"Facebook Profile\"],\n",
        "                    entry[\"Twitter Profile\"],\n",
        "\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        output_workbook.save(excel_output_file_path)\n",
        "        files.download(excel_output_file_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error: {str(e)}\"\n",
        "        print(error_message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NhF6_-HX9gET",
        "outputId": "9522a043-320c-445c-a95f-d976310708a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You selected Search Twitter\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6703c022-a54a-4c8d-9a89-2bad38ade751\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6703c022-a54a-4c8d-9a89-2bad38ade751\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving input.xlsx to input.xlsx\n",
            "User uploaded file \"input.xlsx\" with length 13616 bytes\n",
            "path: input.xlsx\n",
            "0\n",
            "CEO of Xiphos Systems Corporation \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of Xiphos Systems Corporation is Edwin Faier.\n",
            "Error: Not found element FLP8od\n",
            "Edwin Faier- Xiphos Systems Corporation | LinkedIn.\n",
            "CEO name v1: Edwin Faier- Xiphos Systems Corporation | LinkedIn.\n",
            "51\n",
            "Print value ceo: Edwin\n",
            "['Edwin']\n",
            "Print value ceo: Faier-\n",
            "['Edwin', 'Faier-']\n",
            "Print value ceo: Xiphos\n",
            "['Edwin', 'Faier-', 'Xiphos']\n",
            "Print value ceo: Systems\n",
            "['Edwin', 'Faier-', 'Xiphos', 'Systems']\n",
            "Print value ceo: Corporation\n",
            "['Edwin', 'Faier-', 'Xiphos', 'Systems', 'Corporation']\n",
            "Edwin Faier- Xiphos Systems Corporation\n",
            "Edwin Faier- Xiphos Systems Corporation\n",
            "1\n",
            "CEO of Centre for Research in Earth and Space Science (CRESS) \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The context does not define anything about CEO, so I can not extract the requested data from the provided context\n",
            "Using You provider and gpt-3.5-turbo model\n",
            "Error: Not found element FLP8od\n",
            "Error: Not found element hgKElc\n",
            "\n",
            "CEO of Centre for Research in Earth and Space Science (CRESS) \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "This context does not provide any information about the CEO of any company, so I cannot answer this question from the provided context.\n",
            "Using You provider and gpt-3.5-turbo model\n",
            "Error: Not found element FLP8od\n",
            "Error: Not found element hgKElc\n",
            "\n",
            "2\n",
            "CEO of Information Technology Association of Canada (ITAC) \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of ITAC is Angela Mondou.\n",
            "Error: Not found element FLP8od\n",
            "Mississauga, ON (April 16, 2019) – The Information Technology Association of Canada (ITAC), today announced that its National Board of Directors has appointedAngela Mondouas President and Chief Executive Officer, effective immediately.\n",
            "CEO name v1: M\n",
            "1\n",
            "M\n",
            "https://twitter.com/itac_corporate ITAC Corporate (@ITAC_Corporate) / ...\n",
            "3\n",
            "CEO of Northern Lights Aero Foundation \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of Chorus is Lynne McMullen\n",
            "Error: Not found element FLP8od\n",
            "in January 2022. Prior to joining Chorus, Lynne held progressive leadership positions over 20 years with the Seneca School of Aviation.\n",
            "CEO name v1: in January 2022. Prior to joining Chorus, Lynne held progressive leadership positions over 20 years with the Seneca School of Aviation.\n",
            "135\n",
            "in January 2022. Prior to joining Chorus, Lynne held progressive leadership positions over 20 years with the Seneca School of Aviation.\n",
            "in January 2022. Prior to joining Chorus, Lynne held progressive leadership positions over 20 years with the Seneca School of Aviation.\n",
            "https://twitter.com/nlaf09 Northern Lights Aero (@NLAF09) / X\n",
            "4\n",
            "CEO of Gastops Ltd. \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of Gastops Ltd is Shaun Horning\n",
            "Error: Not found element FLP8od\n",
            "INNOVATION, CREATION, AND DIGITIZATION: Meet CEOShaun Horningof Gastops Ltd.\n",
            "CEO name v1: INNOVATION, CREATION, AND DIGITIZATION: Meet CEOShaun Horningof Gastops Ltd.\n",
            "76\n",
            "INNOVATION, CREATION, AND DIGITIZATION: Meet CEOShaun Horningof Gastops Ltd.\n",
            "INNOVATION, CREATION, AND DIGITIZATION: Meet CEOShaun Horningof Gastops Ltd.\n",
            "https://twitter.com/gastops Shaun Horning named President and CEO of Gastops\n",
            "5\n",
            "CEO of KEI Space \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "Juan Tanús\n",
            "Using You provider and gpt-3.5-turbo model\n",
            "Error: Not found element FLP8od\n",
            "Error: Not found element hgKElc\n",
            "\n",
            "CEO of KEI Space \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of Kei Space Design is Juan Tanús.\n",
            "Error: Not found element FLP8od\n",
            "Error: Not found element hgKElc\n",
            "\n",
            "6\n",
            "CEO of Sander Geophysics Limited (SGL) \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of Sander Geophysics is George Sander\n",
            "Error: Not found element FLP8od\n",
            "The company is led by co-presidentsLuise Sanderand Stephan Sander.\n",
            "CEO name v1: The company\n",
            "11\n",
            "The company\n",
            "https://twitter.com/sandergeo Sander Geophysics (@SanderGeo) ...\n",
            "7\n",
            "CEO of BRPH \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of BRPH Companies, Inc. is Brian Curtin.\n",
            "Error: Not found element FLP8od\n",
            "Brian Curtinis the Chairman of the Board, President, and CEO for BRPH Companies, Inc., an employee owned ESOP company providing domestic and international architecture, engineering, construction, and mission critical services for the built environment.\n",
            "CEO name v1: Brian Curtin\n",
            "12\n",
            "Brian Curtin\n",
            "8\n",
            "CEO of Advanced Space \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of Advanced Space is Bradley Cheetham.\n",
            "Error: Not found element FLP8od\n",
            "Bradley Cheethamis an engineer, 3x entrepreneur and lifelong commercial space advocate. He is best known as the co-founder and CEO of Advanced Space where he leads company operations and strategy to deliver flight dynamics and operations solutions to clients across thespace industryspace industryIndia's Space Industry ispredominantly driven by the national Indian Space Research Organisation (ISRO). The industry includes over 500 private suppliers and other various bodies of the Department of Space in all commercial, research and arbitrary regards.https://en.wikipedia.org› wiki › Space_industry_of_IndiaSpace industry of India - Wikipedia.\n",
            "CEO name v1: Bradley Cheetham\n",
            "16\n",
            "Bradley Cheetham\n",
            "https://twitter.com/advancedspace Advanced Space (@AdvancedSpace) ...\n",
            "9\n",
            "CEO of VectorNav Technologies \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "John Brashear\n",
            "Using You provider and gpt-3.5-turbo model\n",
            "Error: Not found element FLP8od\n",
            "John Brashear- President - VectorNav Technologies | LinkedIn.\n",
            "CEO name v1: John Brashear- President - VectorNav Technologies | LinkedIn.\n",
            "61\n",
            "Print value ceo: John\n",
            "['John']\n",
            "Print value ceo: Brashear-\n",
            "['John', 'Brashear-']\n",
            "Print value ceo: President\n",
            "['John', 'Brashear-', 'President']\n",
            "John Brashear- President\n",
            "John Brashear- President\n",
            "https://twitter.com/Johnny_B30 VectorNav Technologies CEO and Key Executive Team\n",
            "10\n",
            "CEO of Kepler Communications Inc. \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of Kepler is Mina Mitry.\n",
            "Error: Not found element FLP8od\n",
            "Mina Mitryis one of Kepler's co-founders and the company's CEO. He holds a master's degree in aerospace engineering from the University of Toronto and has experience conceiving and scaling disruptive companies.\n",
            "CEO name v1: Mina Mitry\n",
            "10\n",
            "Mina Mitry\n",
            "https://twitter.com/mitrymin Mina Mitry (@mitrymin) / X\n",
            "11\n",
            "CEO of Radiant Earth Foundation \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "Jed Sundwall\n",
            "Using You provider and gpt-3.5-turbo model\n",
            "Error: Not found element FLP8od\n",
            "Jed has spent his career working at the intersection of data, product development, cloud computing, economics, and policy. He has helped create data sharing best practices that have been adopted worldwide by NASA, USGS, Google, Microsoft and other institutions around the world.\n",
            "CEO name v1: Jed has spent h\n",
            "15\n",
            "Jed has spent h\n",
            "https://twitter.com/ourradiantearth Radiant Earth (@OurRadiantEarth) / X\n",
            "12\n",
            "CEO of Space Exploration Technologies Corp. (SpaceX) \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of SpaceX is Elon Musk\n",
            "Elon Musk\n",
            "Elon Musk\n",
            "13\n",
            "CEO of Dauria Aerospace \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of Dauria Aerospace is Mikhail Bakirov.\n",
            "Error: Not found element FLP8od\n",
            "Mikhail Bakirovis the CEO of Dauria Aerospace.\n",
            "CEO name v1: Mikhail Bakirov\n",
            "15\n",
            "Mikhail Bakirov\n",
            "14\n",
            "CEO of Skylo Technologies \n",
            "Using Aura provider and gpt-3.5-turbo model\n",
            "Using GeminiProChat provider and gpt-3.5-turbo model\n",
            "The CEO of Skylo Technologies is Parthsarathi “Parth” Trivedi\n",
            "Error: Not found element FLP8od\n",
            "Parthsarathi “Parth” Trivediis the CEO and co-founder of Skylo. Parth is an aerospace engineer, who has led various missions and aerospace projects at MIT sponsored by the US Department of Defense, NASA, and FAA.\n",
            "CEO name v1: Parthsarathi “Parth” Trivedi\n",
            "28\n",
            "Print value ceo: Parthsarathi\n",
            "['Parthsarathi']\n",
            "Print value ceo: “Parth”\n",
            "['Parthsarathi', '“Parth”']\n",
            "Print value ceo: Trivediis\n",
            "['Parthsarathi', '“Parth”', 'Trivediis']\n",
            "Parthsarathi “Parth” Trivediis\n",
            "Parthsarathi “Parth” Trivediis\n"
          ]
        }
      ],
      "source": [
        "driver = web_driver()\n",
        "try:\n",
        "  # @title { run: \"auto\" }\n",
        "  option = \"Search Twitter\" # @param [\"Search Linkedin\", \"Search Facebook\", \"Search Twitter\", \"Find Email\", \"Find All\"]\n",
        "  print('You selected', option)\n",
        "\n",
        "  if option == \"Search Facebook\":\n",
        "    search_facebook_profile()\n",
        "  elif option == \"Search Linkedin\":\n",
        "    search_linkedin_profile()\n",
        "  elif option == \"Find All\":\n",
        "    search_all()\n",
        "  elif option == \"Find Email\":\n",
        "    search_email()\n",
        "  else:\n",
        "    search_twitter_profile()\n",
        "except Exception as e:\n",
        "  error_message = f\"Error: {str(e)}\"\n",
        "  print(error_message)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}